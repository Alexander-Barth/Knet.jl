{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multilayer Perceptron (MLP) \n",
    "(c) Deniz Yuret, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Objectives: See the effect of nonlinearities, learn about regularization and dropout to combat overfitting.\n",
    "* Prerequisites: Linear models (lin.ipynb), AutoGrad, Param, KnetArray, gpu, nll, zeroone\n",
    "* Knet: zeroone, progress, sgd, load, save, gc (used by trainresults)\n",
    "* Knet: xavier, param, param0 (defined and explained)\n",
    "* Knet: Param, KnetArray, gpu (used by param, param0)\n",
    "* Knet: Data, nll, relu, training (used in model definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set display width, load packages, import symbols\n",
    "ENV[\"COLUMNS\"]=72\n",
    "using Pkg; for p in (\"Knet\",\"Plots\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using Knet: Knet, dir, zeroone, progress, sgd, load, save, gc, Param, KnetArray, gpu, Data, nll, relu, training # param, param0, xavier\n",
    "using Statistics: mean\n",
    "using Base.Iterators: flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load data (see 02.mnist.ipynb)\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))  # Load data\n",
    "dtrn,dtst = mnistdata(xsize=(784,:)); # dtrn and dtst = [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For running experiments\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)\n",
    "        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "             for x in takeevery(length(dtrn), progress(sgd(model,repeat(dtrn,100)))))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Some utilities to make model definitions easier:\n",
    "param(d...; init=xavier, atype=atype())=Param(atype(init(d...)))\n",
    "param0(d...; atype=atype())=param(d...; init=zeros, atype=atype)\n",
    "xavier(o,i) = (s = sqrt(2/(i+o)); 2s .* rand(o,i) .- s)\n",
    "atype()=(gpu() >= 0 ? KnetArray{Float32} : Array{Float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A generic multilayer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define a chain of layers\n",
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = nll(c(x),y)\n",
    "(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple linear layers do not improve over a single linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a linear layer (See lin.ipynb):\n",
    "struct Layer0; w; b; end\n",
    "Layer0(i::Int,o::Int) = Layer0(param(o,i),param0(o))\n",
    "(l::Layer0)(x) = (l.w * x .+ l.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here is an example two layer model\n",
    "model=Chain(Layer0(784,64), Layer0(64,10))\n",
    "println.(summary.((l.w,l.b)) for l in model.layers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train the two layer model\n",
    "# 52s [0.240726; 0.281965; 0.0691833; 0.0794]\n",
    "mlp1 = trainresults(\"mlp113a.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here is a single layer (linear) model\n",
    "model=Chain(Layer0(784,10))\n",
    "println.(summary.((l.w,l.b)) for l in model.layers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train the single layer (linear) model\n",
    "# 43s [0.242353; 0.267041; 0.0669667; 0.0749]\n",
    "lin1 = trainresults(\"mlp113b.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Plots; default(fmt=:png,ls=:auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# multilinear converges to a similar solution, not identical because problem is non-convex\n",
    "plot([lin1[1,:], lin1[2,:], mlp1[1,:], mlp1[2,:]], ylim=(0.0,0.4),\n",
    "    labels=[:trnLin :tstLin :trnMulti :tstMulti],xlabel=\"Epochs\",ylabel=\"Loss\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# error results also close to the linear model\n",
    "plot([lin1[3,:], lin1[4,:], mlp1[3,:], mlp1[4,:]], ylim=(0.0,0.1),\n",
    "    labels=[:trnLin,:tstLin,:trnMulti,:tstMulti], xlabel=\"Epochs\", ylabel=\"Error\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple linear layers are useless because they are equivalent to a single linear layer\n",
    "If we write down what is being computed and do some algebra, we can show that what is being computed is still an affine function of the input, i.e. stacking multiple linear layers does not increase the representational capacity of the model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{p} &= \\mbox{soft}(W_2 (W_1 x + b_1) + b_2) \\\\\n",
    "&= \\mbox{soft}((W_2 W_1)\\, x + W_2 b_1 + b_2) \\\\\n",
    "&= \\mbox{soft}(W x + b)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi Layer Perceptron (MLP) adds non-linearities between layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using nonlinearities between layers results in a model with higher capacity and helps underfitting\n",
    "# relu(x)=max(0,x) is a popular function used for this purpose, it replaces all negative values with zeros.\n",
    "struct Layer1; w; b; f; end\n",
    "Layer1(i::Int,o::Int,f=identity) = Layer1(param(o,i),param0(o),f)\n",
    "(l::Layer1)(x) = l.f.(l.w * x .+ l.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We add a nonlinear activation function to all but the last layer\n",
    "model = Chain(Layer1(784,64,relu), Layer1(64,10))\n",
    "# 54s [0.00612065; 0.0864965; 0.00055; 0.0244]\n",
    "mlp2 = trainresults(\"mlp113c.jld2\", model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP solves underfitting but still has an overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# MLP solves the underfitting problem!  A more serious overfitting problem remains.\n",
    "plot([lin1[1,:], lin1[2,:], mlp2[1,:], mlp2[2,:]], ylim=(0.0,0.4),\n",
    "     labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test error improves from 7.5% to 2.5%!\n",
    "plot([lin1[3,:], lin1[4,:], mlp2[3,:], mlp2[4,:]], ylim=(0.0,0.1),\n",
    "    labels=[:trnLin,:tstLin,:trnMLP,:tstMLP], xlabel=\"Epochs\", ylabel=\"Error\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP with L1/L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We add two new fields for L1 and L2 regularization\n",
    "struct Chain2\n",
    "    layers; λ1; λ2\n",
    "    Chain2(layers...; λ1=0, λ2=0) = new(layers, λ1, λ2)\n",
    "end\n",
    "\n",
    "# The prediction and average loss do not change\n",
    "(c::Chain2)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain2)(d::Data) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The loss function penalizes the L1 and/or L2 norms of parameters during training\n",
    "function (c::Chain2)(x,y)\n",
    "    loss = nll(c(x),y)\n",
    "    if training() # Only apply regularization during training, only to weights, not biases.\n",
    "        c.λ1 != 0 && (loss += c.λ1 * sum(sum(abs, l.w) for l in c.layers))\n",
    "        c.λ2 != 0 && (loss += c.λ2 * sum(sum(abs2,l.w) for l in c.layers))\n",
    "    end\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Chain2(Layer1(784,64,relu), Layer1(64,10); λ1=4f-5)\n",
    "# 61s [0.0259648; 0.0722113; 0.00625; 0.0212]\n",
    "mlp3 = trainresults(\"mlp113d.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# overfitting less, test loss improves from 0.0865 to 0.0722\n",
    "plot([mlp2[1,:], mlp2[2,:], mlp3[1,:], mlp3[2,:]], ylim=(0.0,0.15),\n",
    "     labels=[:trnMLP :tstMLP :trnL1 :tstL1],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# test error also improves: 0.0244 -> 0.0212\n",
    "plot([mlp2[3,:], mlp2[4,:], mlp3[3,:], mlp3[4,:]], ylim=(0.0,0.04),\n",
    "     labels=[:trnMLP :tstMLP :trnL1 :tstL1],xlabel=\"Epochs\",ylabel=\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Knet: dropout\n",
    "@doc dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Dropout is another way to address overfitting\n",
    "struct Layer2; w; b; f; pdrop; end\n",
    "Layer2(i::Int,o::Int,f=identity; pdrop=0) = Layer2(param(o,i),param0(o),f,pdrop)\n",
    "(l::Layer2)(x) = l.f.(l.w * dropout(x,l.pdrop) .+ l.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Chain(Layer2(784,64,relu,pdrop=0.2), Layer2(64,10))\n",
    "# 55s [0.0134416; 0.0672397; 0.00371667; 0.0193]\n",
    "mlp4 = trainresults(\"mlp113e.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# overfitting less, loss results improve 0.0865 -> 0.0672\n",
    "plot([mlp2[1,:], mlp2[2,:], mlp4[1,:], mlp4[2,:]], ylim=(0.0,0.15),\n",
    "     labels=[:trnMLP :tstMLP :trnDrop :tstDrop],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this time error also improves 0.0244 -> 0.0193\n",
    "plot([mlp2[3,:], mlp2[4,:], mlp4[3,:], mlp4[4,:]], ylim=(0.0,0.04),\n",
    "     labels=[:trnMLP :tstMLP :trnDrop :tstDrop],xlabel=\"Epochs\",ylabel=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(mlperr=minimum(mlp2[4,:]),L1err=minimum(mlp3[4,:]),dropouterr=minimum(mlp4[4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(mlploss=minimum(mlp2[2,:]),L1loss=minimum(mlp3[2,:]),dropoutloss=minimum(mlp4[2,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP with larger hidden layer and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The current trend is to use models with higher capacity tempered with dropout\n",
    "model = Chain(Layer2(784,256,relu,pdrop=0.2), Layer2(256,10))\n",
    "# 56s [0.00393102; 0.0491462; 0.0004; 0.0154]\n",
    "mlp = trainresults(\"mlp113f.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Both train and test loss is better with the larger model\n",
    "plot([mlp4[1,:], mlp4[2,:], mlp[1,:], mlp[2,:]],ylim=(0,0.15),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We are down to 1.5% error.\n",
    "plot([mlp4[3,:], mlp4[4,:], mlp[3,:], mlp[4,:]],ylim=(0,0.04),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Error\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "julia.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
