{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Knet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A one layer MLP vs a simple RNN\n",
    "\n",
    "([Elman 1990](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/pdf)) A simple RNN takes the previous hidden state as an extra input, and returns the next hidden state as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison of a single MLP layer and corresponding RNN\n",
    "\n",
    "function tanhmlp(param, input)\n",
    "    weight,bias = param\n",
    "    return tanh(input * weight .+ bias)\n",
    "end\n",
    "\n",
    "function tanhrnn(param, input, state)\n",
    "    weight,bias = param\n",
    "    return tanh(hcat(input, state) * weight .+ bias)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/rnn-vs-mlp.png\" />(<a href=\"https://docs.google.com/drawings/d/1bPttFA0GEh7ti3xoWDma1ZbrQ21eulsPDeVUbPgdA7g/edit?usp=sharing\">image source</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "<img src=\"images/diags.png\" />\n",
    "(<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness\">image source</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long Short-Term Memory (LSTM)\n",
    "([Hochreiter and Schmidhuber, 1997](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf))\n",
    "LSTM is a more sophisticated RNN module that performs better with long-range dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/LSTM3-chain.png\" width=800 />\n",
    "([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "f_t &= \\sigma(W_f\\cdot[h_{t-1},x_t] + b_f) & \\text{forget gate} \\\\\n",
    "i_t &= \\sigma(W_i\\cdot[h_{t-1},x_t] + b_i) & \\text{input gate} \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C\\cdot[h_{t-1},x_t] + b_C) & \\text{cell candidate} \\\\\n",
    "C_t &= f_t \\ast C_{t-1} + i_t \\ast \\tilde{C}_t & \\text{new cell} \\\\\n",
    "o_t &= \\sigma(W_o\\cdot[h_{t-1},x_t] + b_o) & \\text{output gate} \\\\\n",
    "h_t &= o_t \\ast \\tanh(C_t) & \\text{new output}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A pure Julia LSTM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function lstm(param, input, state)\n",
    "    weight,bias = param\n",
    "    hidden,cell = state\n",
    "    h       = size(hidden,2)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    forget  = sigm.(gates[:,1:h])\n",
    "    ingate  = sigm.(gates[:,1+h:2h])\n",
    "    outgate = sigm.(gates[:,1+2h:3h])\n",
    "    change  = tanh.(gates[:,1+3h:4h])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh.(cell)\n",
    "    return (hidden,cell)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "\n",
    "Introduced by <a href=\"http://arxiv.org/pdf/1406.1078v3.pdf\">Cho, et al. (2014)</a>, GRU combines the forget and input gates into a single “update gate.”\n",
    "\n",
    "<img src=\"images/LSTM3-var-GRU.png\" />\n",
    "(<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">image source</a>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A more efficient cuDNN based implementation: Knet.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1186\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x, h...; hout, batchSizes) => y\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step, its first dimension is H for unidirectional and 2H for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`.\n",
       "\n",
       "**Hidden states:** Additional arguments to rnn, `h...`, can optionally be used to specify the initial hidden and cell states (cell state is only used by LSTMs). Initial states are assumed to be zero if not specified.  Final hidden states (for all layers) will be discarded if `hout=nothing` (default), or pushed into `hout` (which should be Any[]) otherwise. All hidden and cell states have dimensionality (H,B,L) for unidirectional and (H,B,2L) for bidirectional RNNs.  If `batchSizes` is used and minibatch sizes change over time, B is always taken to be the size of the first minibatch for hidden sizes.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Ignored if `numLayers==1`.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x, h...; hout, batchSizes) => y\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer for each time step.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength. By default a 1-D \u001b[36mx\u001b[39m represents a single instance for a\n",
       "  single time step, a 2-D \u001b[36mx\u001b[39m represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m represents a sequence of identically sized minibatches\n",
       "  for multiple time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer for multi-layer RNNs) for each time step, its first dimension is H\n",
       "  for unidirectional and 2H for bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an array of\n",
       "  (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths).\n",
       "  In this case \u001b[36mx\u001b[39m will typically be 2-D with the second dimension representing variable size batches for time steps. If \u001b[36mbatchSizes\u001b[39m is used,\n",
       "  \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x) ÷ size(x,1)\u001b[39m.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m Additional arguments to rnn, \u001b[36mh...\u001b[39m, can optionally be used to specify the initial hidden and cell states (cell state is only used by\n",
       "  LSTMs). Initial states are assumed to be zero if not specified. Final hidden states (for all layers) will be discarded if \u001b[36mhout=nothing\u001b[39m (default),\n",
       "  or pushed into \u001b[36mhout\u001b[39m (which should be Any[]) otherwise. All hidden and cell states have dimensionality (H,B,L) for unidirectional and (H,B,2L) for\n",
       "  bidirectional RNNs. If \u001b[36mbatchSizes\u001b[39m is used and minibatch sizes change over time, B is always taken to be the size of the first minibatch for hidden\n",
       "  sizes.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Ignored if \u001b[36mnumLayers==1\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R\n",
       "  and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet: RNN\n",
    "@doc RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
