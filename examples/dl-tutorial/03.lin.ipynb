{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Knet, Plots, JLD2, NBInclude\n",
    "gpu(0) #REMOVE\n",
    "@nbinclude(\"02.mnist.ipynb\")  # loads MNIST, defines dtrn,dtst,Atype,train,softmax,zeroone\n",
    "ENV[\"COLUMNS\"]=80         # column width for array printing\n",
    "#broken Plots.plotlyjs()               # for interactive plots\n",
    "Plots.scalefontsizes(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The predict function returns a score for each class as a linear function of input x\n",
    "function linear(w,x)\n",
    "    y = w[1]*mat(x) .+ w[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{KnetArray{Float32,2},1}:\n",
       " KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Nothing} @0x000001020e800000, 31360, 0, #undef), (10, 784))\n",
       " KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Nothing} @0x000001020ea00000, 40, 0, #undef), (10, 1))     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights as a list of [ weightMatrix, biasVector ]\n",
    "winit1(;std=0.01)=map(Atype, [ std*randn(10,784), zeros(10,1) ]);\n",
    "w = winit1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy and zero-one loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"28×28×1×100 KnetArray{Float32,4}\", \"100-element Array{UInt8,1}\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab a minibatch\n",
    "x,y = first(dtst)\n",
    "summary.((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Array{Float32,2}:\n",
       "  0.020949      0.0204775   -0.0932485   …  -0.0769949   -0.0152675\n",
       "  0.0455851    -0.0707735   -0.0228169       0.00569148   0.0522016\n",
       " -0.000504205  -0.214745     0.00253635     -0.082528    -0.107451 \n",
       "  0.132813     -0.103564     0.0237599       0.0487591   -0.0231088\n",
       "  0.0351397    -0.0638922   -0.109847       -0.0460735   -0.100915 \n",
       " -0.0180563    -0.0725433   -0.08814     …   0.00757484  -0.13144  \n",
       "  0.0370442     0.00319698  -0.0865752       0.079252     0.0637138\n",
       " -0.010542      0.0478139    0.0305943       0.107525     0.138668 \n",
       " -0.0714492    -0.124502    -0.00765008     -0.00235417   0.026818 \n",
       "  0.173877      0.296371     0.0476608       0.148999     0.268136 "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random model and calculate predictions for one minibatch\n",
    "Knet.seed!(9)           # for replicability\n",
    "w = winit1()         # random weight matrix and a zero bias vector\n",
    "ypred = linear(w,x)  # predict\n",
    "Array(ypred)         # predictions are given as a 10xN score matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×100 LinearAlgebra.Adjoint{UInt8,Array{UInt8,1}}:\n",
       " 0x07  0x02  0x01  0x0a  0x04  0x01  …  0x01  0x04  0x01  0x07  0x06  0x09"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y'  # correct answers are given as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching findmax(::Array{Float32,2}, ::Int64)\nClosest candidates are:\n  findmax(!Matched::SparseArrays.SparseMatrixCSC{Tv,Ti}, ::Any) where {Tv, Ti} at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.0/SparseArrays/src/sparsematrix.jl:1897\n  findmax(::AbstractArray; dims) at reducedim.jl:810\n  findmax(::Any) at array.jl:2050",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching findmax(::Array{Float32,2}, ::Int64)\nClosest candidates are:\n  findmax(!Matched::SparseArrays.SparseMatrixCSC{Tv,Ti}, ::Any) where {Tv, Ti} at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.0/SparseArrays/src/sparsematrix.jl:1897\n  findmax(::AbstractArray; dims) at reducedim.jl:810\n  findmax(::Any) at array.jl:2050",
      "",
      "Stacktrace:",
      " [1] #accuracy#651(::Bool, ::Function, ::KnetArray{Float32,2}, ::Array{UInt8,1}, ::Int64) at /data/scratch/deniz/.julia/dev/Knet/src/loss.jl:188",
      " [2] accuracy(::KnetArray{Float32,2}, ::Array{UInt8,1}, ::Int64) at /data/scratch/deniz/.julia/dev/Knet/src/loss.jl:187 (repeats 2 times)",
      " [3] top-level scope at none:0"
     ]
    }
   ],
   "source": [
    "# accuracy gives percentage of correct answers\n",
    "accuracy(ypred,y)        # 2-arg version: accuracy on this batch of 100 with initial w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy(w,dtst,linear)  # 3-arg version: accuracy on the whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# zeroone loss (error) defined as 1 - accuracy\n",
    "zeroone(w,data,model) = 1 - accuracy(w,data,model)\n",
    "zeroone(w,dtst,linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate softmax (cross entropy, negative log likelihood) loss of a model with weights w for one minibatch (x,y)\n",
    "# Predict specifies the function for model output: ypred = predict(w,x;o...)\n",
    "softmax(w,x,y,predict; o...)=nll(predict(w,x;o...),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# per-instance average softloss for the first test minibatch, should be close to -log(1/10)=2.3\n",
    "softmax(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Manual loss calculation\n",
    "ypred=linear(w,x)\n",
    "yp1 = exp.(ypred)\n",
    "yp2 = yp1 ./ sum(yp1,1)\n",
    "yp3 = -log.(yp2)\n",
    "yc1 = full(sparse(y,1:100,1f0))\n",
    "sum(Array(yp3).*yc1) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "softmax(w,data,predict) = mean(softmax(w,x,y,predict) for (x,y) in data)\n",
    "softmax(w,dtst,linear)  # per-instance average softmax loss for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the gradient using Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Automatically defined gradient for softloss\n",
    "softgrad = grad(softmax)  # softgrad returns gradient wrt 1st arg w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "setseed(9)\n",
    "w1 = winit1(std=0.1)  # use a larger std to get a larger gradient for this example\n",
    "size.(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g1 = softgrad(w1,x,y,linear)  # gradient has the same size and shape as the first parameter\n",
    "size.(g1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the gradient using numerical approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Array(g1[2])'  \n",
    "# Meaning of gradient:\n",
    "# If I move the last entry of w[2] by epsilon, loss will go up by 0.345075 epsilon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Array(w1[2])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "softmax(w1,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1[2][10] = 0.1   # to numerically check the gradient let's move the last entry by +0.1.\n",
    "Array(w1[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "softmax(w1,x,y,linear)  \n",
    "# We see that the loss moves by +0.03 as expected.\n",
    "# You should check all/most entries in your gradients this way to make sure they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the gradient using manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Manually defined gradient for softloss\n",
    "function softgrad_manual(w,x,y,predict)\n",
    "    x = mat(x)\n",
    "    p = predict(w,x)\n",
    "    p = p .- maximum(p,1) # for numerical stability\n",
    "    expp = exp.(p)\n",
    "    p = expp ./ sum(expp,1)\n",
    "    q = oftype(p, sparse(convert(Vector{Int},y),1:length(y),1,size(p,1),length(y)))\n",
    "    dJdy = (p - q) / size(x,2)\n",
    "    dJdw = dJdy * x'\n",
    "    dJdb = sum(dJdy,2)\n",
    "    Any[dJdw,dJdb]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "g2 = softgrad_manual(w1,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "isapprox(g1[1],g2[1],rtol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "isapprox(g1[2],g2[2],rtol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (SGD) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train model(w) with SGD and return a list containing w for every epoch\n",
    "function train(w,data,predict; epochs=100,lr=0.1,o...)\n",
    "    weights = Any[deepcopy(w)]\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = softgrad(w,x,y,predict;o...)\n",
    "            update!(w,g,lr=lr)  # w[i] = w[i] - lr * g[i]\n",
    "        end\n",
    "        push!(weights,deepcopy(w))\n",
    "    end\n",
    "    return weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the linear model and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if !isfile(\"lin.jld\")\n",
    "    setseed(1)\n",
    "    @time weights = train(winit1(),dtrn,linear,lr=0.1)           # 31.1s\n",
    "    @time trnloss = [ softmax(w,dtrn,linear) for w in weights ]  # 22.2s\n",
    "    @time tstloss = [ softmax(w,dtst,linear) for w in weights ]  # 3.7s\n",
    "    @time trnerr  = [ zeroone(w,dtrn,linear) for w in weights ]  # 20.6s\n",
    "    @time tsterr  = [ zeroone(w,dtst,linear) for w in weights ]  # 3.4s\n",
    "    @save \"lin.jld\" weights trnloss tstloss trnerr tsterr\n",
    "else\n",
    "    @eval (@load \"lin.jld\")\n",
    "end\n",
    "minimum(tstloss),minimum(tsterr)  # 0.2667, 0.0744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot([trnloss tstloss],ylim=(.2,.36),labels=[:trnloss :tstloss],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# Demonstrates underfitting: training loss not close to 0\n",
    "# Also slight overfitting: test loss higher than train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot([trnerr tsterr],ylim=(.06,.10),labels=[:trnerr :tsterr],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# this is the error plot, we get to about 7.5% test error, i.e. 92.5% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for t in logspace(0,2,20)\n",
    "    i = ceil(Int,t)\n",
    "    w = weights[i]\n",
    "    w1 = reshape(Array(w[1])', (28,28,1,10))\n",
    "    w2 = clamp.(w1.+0.5,0,1)\n",
    "    IJulia.clear_output(true)\n",
    "    display(hcat([mnistview(w2,i) for i=1:10]...))\n",
    "    display(\"Epoch $i\")\n",
    "    sleep(1) # (0.96^i)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
