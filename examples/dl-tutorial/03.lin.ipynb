{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models, loss functions, gradients, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Objectives: Define, train and visualize a simple model; understand gradients and SGD; learn to use the GPU.\n",
    "* Prerequisites: [Callable objects](https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects-1)\n",
    "* AutoGrad: Param, @diff, gradient, value (used and explained)\n",
    "* Knet: accuracy, zeroone, train! (defined and explained)\n",
    "* Knet: nll, gpu, KnetArray (used and explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Knet: Knet, Data, minibatch, nll, gpu, KnetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data\",\"mnist.jl\"))  # Load data (see 02.mnist.ipynb)\n",
    "dtrn,dtst = mnistdata(xsize=(784,:),xtype=Array{Float32});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We will use a callable object to define our model \n",
    "# (see https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects-1)\n",
    "struct Linear; w; b; end           # A linear model has two components: w=weightMatrix, b=biasVector.\n",
    "(f::Linear)(x) = f.w * x .+ f.b    # When we use a Linear object like a function it gives us a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the first minibatch from the test set\n",
    "x,y = first(dtst)\n",
    "summary.((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a random Linear model\n",
    "f = Linear(randn(10,784)*0.01, zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display its prediction on the first minibatch\n",
    "ENV[\"COLUMNS\"]=92\n",
    "ypred = f(x)          # predictions are given as a 10xN score matrix         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y'                    # correct answers are given as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can calculate the accuracy of our model for the first minibatch\n",
    "using Statistics\n",
    "accuracy(ypred,y) = mean(y' .== map(i->i[1],findmax(Array(ypred),dims=1)[2]))\n",
    "accuracy(ypred,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can calculate the accuracy of our model for the whole test set\n",
    "accuracy(f,d::Data) = mean(accuracy(f(x),y) for (x,y) in d)\n",
    "accuracy(f,dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ZeroOne loss (or error) is defined as 1 - accuracy\n",
    "zeroone(x...) = 1 - accuracy(x...)\n",
    "zeroone(f,dtst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate negative log likelihood (aka cross entropy, softmax loss) of our model for the first minibatch\n",
    "nll(f(x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Here is what the `nll` function does behind the scenes:\n",
    "using SparseArrays\n",
    "ypred=f(x)\n",
    "yp1 = exp.(ypred)\n",
    "yp2 = yp1 ./ sum(yp1,dims=1)\n",
    "yp3 = -log.(yp2)\n",
    "yc1 = Array(sparse(y,1:100,1f0))\n",
    "sum(Array(yp3).*yc1) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# per-instance average negative log likelihood for the whole test set\n",
    "nll(f,d::Data) = mean(nll(f(x),y) for (x,y) in d)\n",
    "nll(f,dtst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the gradient using AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AutoGrad\n",
    "@doc AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# To compute gradients we need to mark fields of f as Params:\n",
    "f = Linear(Param(randn(10,784)), Param(zeros(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still do predictions with f and calculate loss:\n",
    "nll(f(x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can do the same loss calculation also computing gradients:\n",
    "J = @diff nll(f(x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the actual loss value from J:\n",
    "value(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the gradient of a parameter from J:\n",
    "∇w = gradient(J,f.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Note that each gradient has the same size and shape as the corresponding parameter:\n",
    "∇b = gradient(J,f.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the gradient using numerical approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Meaning of gradient: If I move the last entry of f.b by epsilon, loss will go up by 0.792576 epsilon!\n",
    "@show ∇b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@show f.b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nll(f(x),y)     # loss for the first minibatch with the original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f.b[10] = 0.1   # to numerically check the gradient let's move the last entry of f.b by +0.1.\n",
    "@show f.b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nll(f(x),y)     # We see that the loss moves by ≈ +0.79*0.1 as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.b[10] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the gradient using manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Without AutoGrad we would have to define the gradients manually:\n",
    "function nllgrad(f,x,y)\n",
    "    p = f.w * x .+ f.b\n",
    "    p = p .- maximum(p,dims=1) # for numerical stability\n",
    "    expp = exp.(p)\n",
    "    p = expp ./ sum(expp,dims=1)\n",
    "    q = oftype(p, sparse(convert(Vector{Int},y),1:length(y),1,size(p,1),length(y)))\n",
    "    dJdy = (p - q) / size(x,2)\n",
    "    dJdw = dJdy * x'\n",
    "    dJdb = vec(sum(dJdy,dims=2))\n",
    "    dJdw,dJdb\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "∇w2,∇b2 = nllgrad(f,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "∇w2 ≈ ∇w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "∇b2 ≈ ∇b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training with Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra: axpy!\n",
    "\n",
    "function train!(model, data; lr=0.1)\n",
    "    for (x,y) in data\n",
    "        loss = @diff nll(model(x),y)\n",
    "        for param in (model.w, model.b)\n",
    "            ∇param = gradient(loss, param)\n",
    "            axpy!(-lr, ∇param, value(param))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a randomly initialized model for 10 epochs\n",
    "model = Linear(Param(randn(10,784)*0.01), Param(zeros(10)))\n",
    "@show nll(model,dtst)\n",
    "@time for i=1:10; train!(model,dtrn); end\n",
    "@show nll(model,dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work on the GPU, all we have to do is convert our Arrays to KnetArrays:\n",
    "if Knet.gpu() >= 0\n",
    "    dtrn.xtype = dtst.xtype = KnetArray{Float32}\n",
    "    model = Linear(Param(KnetArray{Float32}(randn(10,784)*0.01)), Param(KnetArray{Float32}(zeros(10))))\n",
    "    @show nll(model,dtst)\n",
    "    @time for i=1:10; train!(model,dtrn); end\n",
    "    @show nll(model,dtst)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect some data to draw training curves and visualizing weights:\n",
    "using FileIO\n",
    "if !isfile(\"lin.jld2\")\n",
    "    models = []; trnloss = []; tstloss = []; trnerr = []; tsterr = []\n",
    "    model = Linear(Param(KnetArray{Float32}(randn(10,784)*0.01)), Param(KnetArray{Float32}(zeros(10))))\n",
    "    @time while true\n",
    "        push!(models, deepcopy(model))\n",
    "        push!(trnloss, nll(model,dtrn))\n",
    "        push!(tstloss, nll(model,dtst))\n",
    "        push!(trnerr, zeroone(model,dtrn))\n",
    "        push!(tsterr, zeroone(model,dtst))\n",
    "        length(tsterr) == 100 && break\n",
    "        train!(model,dtrn)\n",
    "    end\n",
    "    save(\"lin.jld2\",\"trnloss\",trnloss,\"tstloss\",tstloss,\"trnerr\",trnerr,\"tsterr\",tsterr)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = load(\"lin.jld2\")\n",
    "minimum(lin[\"tstloss\"]), minimum(lin[\"tsterr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear model shows underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot([lin[\"trnloss\"], lin[\"tstloss\"]],ylim=(.0,.4),labels=[:trnloss :tstloss],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# Demonstrates underfitting: training loss not close to 0\n",
    "# Also slight overfitting: test loss higher than train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot([lin[\"trnerr\"], lin[\"tsterr\"]],ylim=(.0,.12),labels=[:trnerr :tsterr],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# this is the error plot, we get to about 7.5% test error, i.e. 92.5% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let us visualize the evolution of the weight matrix as images below\n",
    "# Each row is turned into a 28x28 image with positive numbers light and negative numbers dark gray\n",
    "using Images, ImageMagick\n",
    "mnistview(x,i)=colorview(Gray,permutedims(x[:,:,1,i],(2,1)))\n",
    "for t in 10 .^ range(0,stop=log10(length(models)),length=10) #logspace(0,2,20)\n",
    "    i = floor(Int,t)\n",
    "    f = models[i]\n",
    "    w1 = reshape(Array(value(f.w))', (28,28,1,10))\n",
    "    w2 = clamp.(w1.+0.5,0,1)\n",
    "    IJulia.clear_output(true)\n",
    "    display(hcat([mnistview(w2,i) for i=1:10]...))\n",
    "    display(\"Epoch $i\")\n",
    "    sleep(1) # (0.96^i)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
