{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, JLD, NBInclude\n",
    "nbinclude(\"mnist.ipynb\")  # loads MNIST, defines dtrn,dtst,Atype,train,softmax,zeroone\n",
    "lin = load(\"lin.jld\")     # loads linear model results for comparison\n",
    "ENV[\"COLUMNS\"]=80         # column width for array printing\n",
    "plotlyjs();               # for interactive plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear layers do not improve over linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try to concatenate multiple linear layers\n",
    "function multilinear(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*mat(x) .+ w[i+1]\n",
    "    end\n",
    "    return x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization for multiple layers: h=array of layer sizes\n",
    "# Output is an array [w0,b0,w1,b1,...,wn,bn] where wi,bi is the weight matrix and bias vector for the i'th layer\n",
    "function winit(h...)  # use winit(x,h1,h2,...,hn,y) for n hidden layer model\n",
    "    w = Any[]\n",
    "    for i=2:length(h)\n",
    "        push!(w, xavier(h[i],h[i-1]))\n",
    "        push!(w, zeros(h[i],1))\n",
    "    end\n",
    "    map(Atype, w)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Knet.KnetArray{Float32,2},1}:\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081057e0000, 200704, 0, nothing), (64, 784))\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053eca00, 256, 0, nothing), (64, 1))     \n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081058e0000, 2560, 0, nothing), (10, 64))   \n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053ecc00, 40, 0, nothing), (10, 1))      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w64=winit(784,64,10) # gives weights and biases for a multi layer model with a single hidden layer of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3366258f0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x,y) = first(dtst)\n",
    "softmax(w64,x,y,multilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2855976f0, 0.07950000000000002)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !isfile(\"mlp1.jld\")\n",
    "    setseed(1)\n",
    "    @time weightsML=train(winit(784,64,10),dtrn,multilinear,lr=0.1)       # 33.9s\n",
    "    @time trnlossML = [ softmax(w,dtrn,multilinear) for w in weightsML ]  # 22.2s\n",
    "    @time tstlossML = [ softmax(w,dtst,multilinear) for w in weightsML ]  # 3.73s\n",
    "    @time trnerrML =  [ zeroone(w,dtrn,multilinear) for w in weightsML ]  # 22.8s\n",
    "    @time tsterrML =  [ zeroone(w,dtst,multilinear) for w in weightsML ]  # 3.84s\n",
    "    @save \"mlp1.jld\" weightsML trnlossML tstlossML trnerrML tsterrML\n",
    "else\n",
    "    @eval (@load \"mlp1.jld\")\n",
    "end\n",
    "minimum(tstlossML),minimum(tsterrML)  # 0.2856, 0.0795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([lin[\"trnloss\"] lin[\"tstloss\"] trnlossML tstlossML],ylim=(0.2,0.4),\n",
    "    labels=[:trnLin :tstLin :trnMulti :tstMulti],xlabel=\"Epochs\",ylabel=\"Loss\")  \n",
    "# multilinear converges to a similar solution, not identical because problem is non-convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([lin[\"trnerr\"] lin[\"tsterr\"] trnerrML tsterrML],ylim=(0.06,0.12),\n",
    "    labels=[:trnLin :tstLin :trnMulti :tstMulti],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# error results also close to the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsML = nothing; knetgc() # to save gpu memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear layers are useless because they are equivalent to a single linear layer\n",
    "If we write down what is being computed and do some algebra, we can show that what is being computed is still an affine function of the input, i.e. stacking multiple linear layers does not increase the representational capacity of the model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{p} &= \\mbox{soft}(W_2 (W_1 x + b_1) + b_2) \\\\\n",
    "&= \\mbox{soft}((W_2 W_1)\\, x + W_2 b_1 + b_2) \\\\\n",
    "&= \\mbox{soft}(W x + b)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP) adds non-linearities between layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nonlinearities (relu) results in a model with higher capacity which helps underfitting\n",
    "function mlp(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*mat(x) .+ w[i+1]\n",
    "        if i < length(w)-1  # Apply element-wise non-linearity after every layer except the last\n",
    "            x = relu.(x)    # relu here is the only difference between this and multilinear\n",
    "        end    \n",
    "    end\n",
    "    return x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Knet.KnetArray{Float32,2},1}:\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x0000008105842000, 200704, 0, nothing), (64, 784))\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053ee600, 256, 0, nothing), (64, 1))     \n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081058e4400, 2560, 0, nothing), (10, 64))   \n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053ee800, 40, 0, nothing), (10, 1))      "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w64=winit(784,64,10) # gives weights and biases for an MLP with a single hidden layer of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3136413f0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(w64,x,y,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38.562059 seconds (33.81 M allocations: 18.993 GiB, 6.97% gc time)\n",
      " 25.646538 seconds (8.54 M allocations: 18.050 GiB, 8.03% gc time)\n",
      "  4.297011 seconds (1.42 M allocations: 3.008 GiB, 8.11% gc time)\n",
      " 24.938192 seconds (8.20 M allocations: 18.565 GiB, 8.52% gc time)\n",
      "  4.159445 seconds (1.37 M allocations: 3.095 GiB, 8.53% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.080823354f0, 0.023499999999999965)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !isfile(\"mlp2.jld\")\n",
    "    setseed(1)\n",
    "    @time weightsMLP=train(winit(784,64,10),dtrn,mlp,lr=0.1)        # 35.4s\n",
    "    @time trnlossMLP = [ softmax(w,dtrn,mlp) for w in weightsMLP ]  # 23.7s\n",
    "    @time tstlossMLP = [ softmax(w,dtst,mlp) for w in weightsMLP ]  # 3.99s\n",
    "    @time trnerrMLP =  [ zeroone(w,dtrn,mlp) for w in weightsMLP ]  # 23.3s\n",
    "    @time tsterrMLP =  [ zeroone(w,dtst,mlp) for w in weightsMLP ]  # 3.91s\n",
    "    @save \"mlp2.jld\" weightsMLP trnlossMLP tstlossMLP trnerrMLP tsterrMLP\n",
    "else\n",
    "    @eval (@load \"mlp2.jld\")\n",
    "end\n",
    "minimum(tstlossMLP),minimum(tsterrMLP)  # 0.0808, 0.0235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP solves underfitting but still has an overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([lin[\"trnloss\"] lin[\"tstloss\"] trnlossMLP tstlossMLP],ylim=(0.0,0.4),\n",
    "    labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Loss\")  \n",
    "# Solves the underfitting problem!\n",
    "# A more serious overfitting problem remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([lin[\"trnerr\"] lin[\"tsterr\"] trnerrMLP tsterrMLP],ylim=(0,0.1),\n",
    "    labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# test error improves from 7.5% to 2.3%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsMLP = nothing; knetgc() # to save gpu memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine softmax loss function to accept keyword parameters l1 and l2 for regularization\n",
    "# Use non-zero l1 or l2 for regularization (only on matrices not biases)\n",
    "function softmax(w,x,y,predict;l1=0,l2=0,o...)\n",
    "    J = nll(predict(w,x;o...),y)\n",
    "    if l1 != 0; J += Float32(l1) * sum(sum(abs,wi)  for wi in w[1:2:end]); end\n",
    "    if l2 != 0; J += Float32(l2) * sum(sum(abs2,wi) for wi in w[1:2:end]); end\n",
    "    return J\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51.794006 seconds (48.03 M allocations: 19.536 GiB, 5.74% gc time)\n",
      " 26.723961 seconds (8.53 M allocations: 18.049 GiB, 8.10% gc time)\n",
      "  4.462811 seconds (1.42 M allocations: 3.008 GiB, 8.19% gc time)\n",
      " 25.088010 seconds (8.19 M allocations: 18.565 GiB, 8.77% gc time)\n",
      "  4.169910 seconds (1.37 M allocations: 3.095 GiB, 8.77% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.07594314f0, 0.02200000000000002)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We still have overfitting, let's try L1 regularization\n",
    "if !isfile(\"mlp3.jld\")\n",
    "    srand(1)\n",
    "    @time weightsL1=train(winit(784,64,10),dtrn,mlp;lr=0.1,l1=0.00004)  # 47.3s\n",
    "    @time trnlossL1= [ softmax(w,dtrn,mlp) for w in weightsL1 ]  # 24.8s\n",
    "    @time tstlossL1= [ softmax(w,dtst,mlp) for w in weightsL1 ]  # 4.17s\n",
    "    @time trnerrL1=  [ zeroone(w,dtrn,mlp) for w in weightsL1 ]  # 23.7s\n",
    "    @time tsterrL1=  [ zeroone(w,dtst,mlp) for w in weightsL1 ]  # 3.95s\n",
    "    @save \"mlp3.jld\" weightsL1 trnlossL1 tstlossL1 trnerrL1 tsterrL1\n",
    "else\n",
    "    @eval (@load \"mlp3.jld\")\n",
    "end\n",
    "minimum(tstlossL1),minimum(tsterrL1)  # 0.0759, 0.0220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnlossMLP tstlossMLP trnlossL1 tstlossL1],ylim=(0,0.15),\n",
    "    labels=[:trnMLP :tstMLP :trnL1 :tstL1],xlabel=\"Epochs\", ylabel=\"Loss\")  \n",
    "# overfitting less, test loss improves from 0.0808 to 0.0759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerrMLP tsterrMLP trnerrL1 tsterrL1],ylim=(0,0.04),\n",
    "    labels=[:trnMLP :tstMLP :trnL1 :tstL1],xlabel=\"Epochs\", ylabel=\"Error\")    \n",
    "# however test error does not change significantly: 0.0235 -> 0.0220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsL1 = nothing; knetgc() # to save gpu memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout is another way to address overfitting\n",
    "function mlpdrop(w,x; pdrop=(0,0))\n",
    "    for i=1:2:length(w)\n",
    "        x = dropout(x, pdrop[i==1?1:2])  # apply one of two dropout rates\n",
    "        x = w[i]*mat(x) .+ w[i+1]\n",
    "        if i < length(w)-1; x = relu.(x); end\n",
    "    end\n",
    "    return x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "dropout(x, p)\n",
       "```\n",
       "\n",
       "Given an array `x` and probability `0<=p<=1`, just return `x` if `p==0`, or return an array `y` in which each element is 0 with probability `p` or `x[i]/(1-p)` with probability `1-p`.  Use `seed::Number` to set the random number seed for reproducible results. See [(Srivastava et al. 2014)](http://www.jmlr.org/papers/v15/srivastava14a.html) for a reference.\n"
      ],
      "text/plain": [
       "```\n",
       "dropout(x, p)\n",
       "```\n",
       "\n",
       "Given an array `x` and probability `0<=p<=1`, just return `x` if `p==0`, or return an array `y` in which each element is 0 with probability `p` or `x[i]/(1-p)` with probability `1-p`.  Use `seed::Number` to set the random number seed for reproducible results. See [(Srivastava et al. 2014)](http://www.jmlr.org/papers/v15/srivastava14a.html) for a reference.\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42.472094 seconds (37.79 M allocations: 19.173 GiB, 7.05% gc time)\n",
      " 25.777243 seconds (8.57 M allocations: 18.051 GiB, 8.15% gc time)\n",
      "  4.311885 seconds (1.43 M allocations: 3.008 GiB, 8.21% gc time)\n",
      " 24.608715 seconds (8.20 M allocations: 18.565 GiB, 8.29% gc time)\n",
      "  4.139163 seconds (1.37 M allocations: 3.095 GiB, 8.30% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06392153f0, 0.01880000000000004)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !isfile(\"mlp4.jld\")\n",
    "    setseed(1)\n",
    "    @time weightsDR=train(winit(784,64,10),dtrn,mlpdrop;lr=0.1,pdrop=(0.2,0))  # 38.9s\n",
    "    @time trnlossDR = [ softmax(w,dtrn,mlpdrop) for w in weightsDR ]     # 25.7s\n",
    "    @time tstlossDR = [ softmax(w,dtst,mlpdrop) for w in weightsDR ]     # 4.25s\n",
    "    @time trnerrDR =  [ zeroone(w,dtrn,mlpdrop) for w in weightsDR ]     # 24.3s\n",
    "    @time tsterrDR =  [ zeroone(w,dtst,mlpdrop) for w in weightsDR ]     # 4.11s\n",
    "    @save \"mlp4.jld\" weightsDR trnlossDR tstlossDR trnerrDR tsterrDR\n",
    "else\n",
    "    @eval (@load \"mlp4.jld\")\n",
    "end\n",
    "minimum(tstlossDR),minimum(tsterrDR)  # 0.0639, 0.0188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnlossMLP tstlossMLP trnlossDR tstlossDR],ylim=(0,0.15),\n",
    "    labels=[:trnMLP :tstMLP :trnDropout :tstDropout],xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "# overfitting less, loss results improve 0.0808 -> 0.0639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerrMLP tsterrMLP trnerrDR tsterrDR],ylim=(0,0.04),\n",
    "    labels=[:trnMLP :tstMLP :trnDropout :tstDropout],xlabel=\"Epochs\", ylabel=\"Error\")  \n",
    "# this time error also improves 0.0235 -> 0.0188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:mlperr, 0.023499999999999965, :L1err, 0.02200000000000002, :dropouterr, 0.01880000000000004)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ":mlperr,minimum(tsterrMLP),:L1err,minimum(tsterrL1),:dropouterr,minimum(tsterrDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:mlploss, 0.080823354f0, :L1loss, 0.07594314f0, :dropoutloss, 0.06392153f0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ":mlploss,minimum(tstlossMLP),:L1loss,minimum(tstlossL1),:dropoutloss,minimum(tstlossDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsDR = nothing; knetgc() # to save gpu memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with larger hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39.956995 seconds (37.86 M allocations: 19.173 GiB, 6.96% gc time)\n",
      " 24.695883 seconds (8.50 M allocations: 18.047 GiB, 8.38% gc time)\n",
      "  4.158595 seconds (1.42 M allocations: 3.008 GiB, 8.37% gc time)\n",
      " 24.756995 seconds (8.19 M allocations: 18.565 GiB, 8.41% gc time)\n",
      "  4.204890 seconds (1.37 M allocations: 3.095 GiB, 8.32% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04944369f0, 0.014800000000000035)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current trend is to use models with higher capacity tempered with dropout\n",
    "if !isfile(\"mlp.jld\")\n",
    "    setseed(1)\n",
    "    @time weights=train(winit(784,256,10),dtrn,mlpdrop;lr=0.1,pdrop=(0.2,0))  # 34.6s\n",
    "    @time trnloss = [ softmax(w,dtrn,mlpdrop) for w in weights ] # 21.2s\n",
    "    @time tstloss = [ softmax(w,dtst,mlpdrop) for w in weights ] # 3.61s\n",
    "    @time trnerr =  [ zeroone(w,dtrn,mlpdrop) for w in weights ] # 21.7s\n",
    "    @time tsterr =  [ zeroone(w,dtst,mlpdrop) for w in weights ] # 3.63s\n",
    "    @save \"mlp.jld\" weights trnloss tstloss trnerr tsterr\n",
    "else\n",
    "    @eval (@load \"mlp.jld\")\n",
    "end\n",
    "minimum(tstloss),minimum(tsterr)  # 0.0494, 0.0148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnlossDR tstlossDR trnloss tstloss],ylim=(0,0.15),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerrDR tsterrDR trnerr tsterr],ylim=(0,0.04),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Error\")\n",
    "# We are down to 0.015 error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing hidden weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"COLUMNS\"]=120\n",
    "w = weights[end]\n",
    "w1 = reshape(Array(w[1])', (28,28,1,256))\n",
    "w2 = clamp.(2.5.*w1.+0.5,0,1)\n",
    "IJulia.clear_output(true)\n",
    "display(hvcat(16, [mnistview(w2,i) for i=1:256]...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
