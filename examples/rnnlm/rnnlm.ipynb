{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN language model\n",
    "Loosely based on [Zaremba et al. 2014](https://arxiv.org/abs/1409.2329), this example trains a word based RNN language model on Mikolov's PTB data with 10K vocab. It uses the `batchSizes` feature of `rnnforw` to process batches with different sized sentences. The `mb` minibatching function sorts sentences in a corpus by length and tries to group similarly sized sentences together. For an example that uses fixed length batches and goes across sentence boundaries see the [charlm](https://github.com/denizyuret/Knet.jl/blob/master/examples/charlm/charlm.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet\n",
    "EPOCHS=10\n",
    "RNNTYPE=:lstm\n",
    "BATCHSIZE=64\n",
    "EMBEDSIZE=128\n",
    "HIDDENSIZE=256\n",
    "VOCABSIZE=10000\n",
    "NUMLAYERS=1\n",
    "DROPOUT=0.5\n",
    "LR=0.001\n",
    "BETA_1=0.9\n",
    "BETA_2=0.999\n",
    "EPS=1e-08;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068-element Array{Array{UInt16,1},1}\n",
      "3370-element Array{Array{UInt16,1},1}\n",
      "3761-element Array{Array{UInt16,1},1}\n",
      "9999-element Array{String,1}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "include(Knet.dir(\"data\",\"mikolovptb.jl\"))\n",
    "(trn,val,tst,vocab) = mikolovptb()\n",
    "@assert VOCABSIZE == length(vocab)+1 # +1 for the EOS token\n",
    "for x in (trn,val,tst,vocab); println(summary(x)); end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UInt16[0x008e, 0x004e, 0x0036, 0x00fb, 0x0938, 0x0195]\n",
      "[\"no\", \"it\", \"was\", \"n't\", \"black\", \"monday\"]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample\n",
    "println(tst[1])\n",
    "println(vocab[tst[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "mikolovptb()\n",
       "```\n",
       "\n",
       "Read [PTB](https://catalog.ldc.upenn.edu/ldc99t42) text from Mikolov's [RNNLM](http://www.fit.vutbr.cz/~imikolov/rnnlm) toolkit which has been lowercased and reduced to a 10K vocabulary size.  Return a tuple (trn,dev,tst,vocab) where\n",
       "\n",
       "```\n",
       "trn::Vector{Vector{UInt16}}: 42068 sentences, 887521 words\n",
       "dev::Vector{Vector{UInt16}}: 3370 sentences, 70390 words\n",
       "tst::Vector{Vector{UInt16}}: 3761 sentences, 78669 words\n",
       "vocab::Vector{String}: 9999 unique words\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  mikolovptb()\u001b[39m\n",
       "\n",
       "  Read PTB (https://catalog.ldc.upenn.edu/ldc99t42) text from Mikolov's RNNLM\n",
       "  (http://www.fit.vutbr.cz/~imikolov/rnnlm) toolkit which has been lowercased\n",
       "  and reduced to a 10K vocabulary size. Return a tuple (trn,dev,tst,vocab)\n",
       "  where\n",
       "\n",
       "\u001b[36m  trn::Vector{Vector{UInt16}}: 42068 sentences, 887521 words\u001b[39m\n",
       "\u001b[36m  dev::Vector{Vector{UInt16}}: 3370 sentences, 70390 words\u001b[39m\n",
       "\u001b[36m  tst::Vector{Vector{UInt16}}: 3761 sentences, 78669 words\u001b[39m\n",
       "\u001b[36m  vocab::Vector{String}: 9999 unique words\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc mikolovptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658, 53, 59)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minibatch data into (x,y,b) triples. This is the most complicated part of the code:\n",
    "# for language models x and y contain the same words shifted, x has an EOS in the beginning, y has an EOS at the end\n",
    "# x,y = [ s11,s21,s31,...,s12,s22,...] i.e. all the first words followed by all the second words etc.\n",
    "# b = [b1,b2,...,bT] i.e. how many sentences have first words, how many have second words etc.\n",
    "# length(x)==length(y)==sum(b) and length(b)=length(s1)+1 (+1 because of EOS)\n",
    "# sentences in batch should be sorted from longest to shortest, i.e. s1 is the longest sentence\n",
    "function mb(sentences,batchsize)\n",
    "    sentences = sort(sentences,by=length,rev=true)\n",
    "    data = []; eos = VOCABSIZE\n",
    "    for i = 1:batchsize:length(sentences)\n",
    "        j = min(i+batchsize-1,length(sentences))\n",
    "        sij = view(sentences,i:j)\n",
    "        T = 1+length(sij[1])\n",
    "        x = UInt16[]; y = UInt16[]; b = UInt16[]\n",
    "        for t=1:T\n",
    "            bt = 0\n",
    "            for s in sij\n",
    "                if t == 1\n",
    "                    push!(x,eos)\n",
    "                    push!(y,s[1])\n",
    "                elseif t <= length(s)\n",
    "                    push!(x,s[t-1])\n",
    "                    push!(y,s[t])\n",
    "                elseif t == 1+length(s)\n",
    "                    push!(x,s[t-1])\n",
    "                    push!(y,eos)\n",
    "                else\n",
    "                    break\n",
    "                end\n",
    "                bt += 1\n",
    "            end\n",
    "            push!(b,bt)\n",
    "        end\n",
    "        push!(data,(x,y,b))\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "mbtrn = mb(trn,BATCHSIZE)\n",
    "mbval = mb(val,BATCHSIZE)\n",
    "mbtst = mb(tst,BATCHSIZE)\n",
    "map(length,(mbtrn,mbval,mbtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "function initmodel()\n",
    "    w(d...)=KnetArray(xavier(Float32,d...))\n",
    "    b(d...)=KnetArray(zeros(Float32,d...))\n",
    "    r,wr = rnninit(EMBEDSIZE,HIDDENSIZE,rnnType=RNNTYPE,numLayers=NUMLAYERS,dropout=DROPOUT)\n",
    "    wx = w(EMBEDSIZE,VOCABSIZE)\n",
    "    wy = w(VOCABSIZE,HIDDENSIZE)\n",
    "    by = b(VOCABSIZE,1)\n",
    "    return r,wr,wx,wy,by\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "rnninit(inputSize, hiddenSize; opts...)\n",
       "```\n",
       "\n",
       "Return an `(r,w)` pair where `r` is a RNN struct and `w` is a single weight array that includes all matrices and biases for the RNN. Keyword arguments:\n",
       "\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0.0`: Dropout probability. Ignored if `numLayers==1`.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnninit(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\n",
       "  Return an \u001b[36m(r,w)\u001b[39m pair where \u001b[36mr\u001b[39m is a RNN struct and \u001b[36mw\u001b[39m is a single weight array\n",
       "  that includes all matrices and biases for the RNN. Keyword arguments:\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0.0\u001b[39m: Dropout probability. Ignored if \u001b[36mnumLayers==1\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  RNNs compute the output h[t] for a given iteration from the recurrent input\n",
       "  h[t-1] and the previous layer input x[t] given matrices W, R and biases bW,\n",
       "  bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc rnninit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and its gradient\n",
    "function predict(ws,xs,bs;pdrop=0)\n",
    "    r,wr,wx,wy,by = ws\n",
    "    x = wx[:,xs] # xs=(ΣBt) x=(X,ΣBt)\n",
    "    x = dropout(x,pdrop)\n",
    "    (y,_) = rnnforw(r,wr,x,batchSizes=bs) # y=(H,ΣBt)\n",
    "    y = dropout(y,pdrop)\n",
    "    return wy * y .+ by  # return=(V,ΣBt)\n",
    "end\n",
    "\n",
    "loss(w,x,y,b;o...) = nll(predict(w,x,b;o...), y)\n",
    "\n",
    "lossgradient = gradloss(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "rnnforw(r, w, x[, hx, cx]; batchSizes, hy, cy)\n",
       "```\n",
       "\n",
       "Returns a tuple (y,hyout,cyout,rs) given rnn `r`, weights `w`, input `x` and optionally the initial hidden and cell states `hx` and `cx` (`cx` is only used in LSTMs).  `r` and `w` should come from a previous call to `rnninit`.  Both `hx` and `cx` are optional, they are treated as zero arrays if not provided.  The output `y` contains the hidden states of the final layer for each time step, `hyout` and `cyout` give the final hidden and cell states for all layers, `rs` is a buffer the RNN needs for its gradient calculation.\n",
       "\n",
       "The boolean keyword arguments `hy` and `cy` control whether `hyout` and `cyout` will be output.  By default `hy = (hx!=nothing)` and `cy = (cx!=nothing && r.mode==2)`, i.e. a hidden state will be output if one is provided as input and for cell state we also require an LSTM.  If `hy`/`cy` is `false`, `hyout`/`cyout` will be `nothing`. `batchSizes` can be an integer array that specifies non-uniform batch sizes as explained below. By default `batchSizes=nothing` and the same batch size, `size(x,2)`, is used for all time steps.\n",
       "\n",
       "The input and output dimensions are:\n",
       "\n",
       "  * `x`: (X,[B,T])\n",
       "  * `y`: (H/2H,[B,T])\n",
       "  * `hx`,`cx`,`hyout`,`cyout`: (H,B,L/2L)\n",
       "  * `batchSizes`: `nothing` or `Vector{Int}(T)`\n",
       "\n",
       "where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength, L is numLayers.  `x` can be 1, 2, or 3 dimensional.  If `batchSizes==nothing`, a 1-D `x` represents a single instance, a 2-D `x` represents a single minibatch, and a 3-D `x` represents a sequence of identically sized minibatches.  If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step in the sequence, in which case `sum(batchSizes)` should equal `div(length(x),size(x,1))`. `y` has the same dimensionality as `x`, differing only in its first dimension, which is H if the RNN is unidirectional, 2H if bidirectional.  Hidden vectors `hx`, `cx`, `hyout`, `cyout` all have size (H,B1,L) for unidirectional RNNs, and (H,B1,2L) for bidirectional RNNs where B1 is the size of the first minibatch.\n"
      ],
      "text/plain": [
       "\u001b[36m  rnnforw(r, w, x[, hx, cx]; batchSizes, hy, cy)\u001b[39m\n",
       "\n",
       "  Returns a tuple (y,hyout,cyout,rs) given rnn \u001b[36mr\u001b[39m, weights \u001b[36mw\u001b[39m, input \u001b[36mx\u001b[39m and\n",
       "  optionally the initial hidden and cell states \u001b[36mhx\u001b[39m and \u001b[36mcx\u001b[39m (\u001b[36mcx\u001b[39m is only used in\n",
       "  LSTMs). \u001b[36mr\u001b[39m and \u001b[36mw\u001b[39m should come from a previous call to \u001b[36mrnninit\u001b[39m. Both \u001b[36mhx\u001b[39m and \u001b[36mcx\u001b[39m\n",
       "  are optional, they are treated as zero arrays if not provided. The output \u001b[36my\u001b[39m\n",
       "  contains the hidden states of the final layer for each time step, \u001b[36mhyout\u001b[39m and\n",
       "  \u001b[36mcyout\u001b[39m give the final hidden and cell states for all layers, \u001b[36mrs\u001b[39m is a buffer\n",
       "  the RNN needs for its gradient calculation.\n",
       "\n",
       "  The boolean keyword arguments \u001b[36mhy\u001b[39m and \u001b[36mcy\u001b[39m control whether \u001b[36mhyout\u001b[39m and \u001b[36mcyout\u001b[39m will\n",
       "  be output. By default \u001b[36mhy = (hx!=nothing)\u001b[39m and \u001b[36mcy = (cx!=nothing &&\n",
       "  r.mode==2)\u001b[39m, i.e. a hidden state will be output if one is provided as input\n",
       "  and for cell state we also require an LSTM. If \u001b[36mhy\u001b[39m/\u001b[36mcy\u001b[39m is \u001b[36mfalse\u001b[39m, \u001b[36mhyout\u001b[39m/\u001b[36mcyout\u001b[39m\n",
       "  will be \u001b[36mnothing\u001b[39m. \u001b[36mbatchSizes\u001b[39m can be an integer array that specifies\n",
       "  non-uniform batch sizes as explained below. By default \u001b[36mbatchSizes=nothing\u001b[39m\n",
       "  and the same batch size, \u001b[36msize(x,2)\u001b[39m, is used for all time steps.\n",
       "\n",
       "  The input and output dimensions are:\n",
       "\n",
       "    •    \u001b[36mx\u001b[39m: (X,[B,T])\n",
       "\n",
       "    •    \u001b[36my\u001b[39m: (H/2H,[B,T])\n",
       "\n",
       "    •    \u001b[36mhx\u001b[39m,\u001b[36mcx\u001b[39m,\u001b[36mhyout\u001b[39m,\u001b[36mcyout\u001b[39m: (H,B,L/2L)\n",
       "\n",
       "    •    \u001b[36mbatchSizes\u001b[39m: \u001b[36mnothing\u001b[39m or \u001b[36mVector{Int}(T)\u001b[39m\n",
       "\n",
       "  where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength, L is\n",
       "  numLayers. \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional. If \u001b[36mbatchSizes==nothing\u001b[39m, a 1-D \u001b[36mx\u001b[39m\n",
       "  represents a single instance, a 2-D \u001b[36mx\u001b[39m represents a single minibatch, and a\n",
       "  3-D \u001b[36mx\u001b[39m represents a sequence of identically sized minibatches. If \u001b[36mbatchSizes\u001b[39m\n",
       "  is an array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step in the sequence, in which case \u001b[36msum(batchSizes)\u001b[39m should equal\n",
       "  \u001b[36mdiv(length(x),size(x,1))\u001b[39m. \u001b[36my\u001b[39m has the same dimensionality as \u001b[36mx\u001b[39m, differing only\n",
       "  in its first dimension, which is H if the RNN is unidirectional, 2H if\n",
       "  bidirectional. Hidden vectors \u001b[36mhx\u001b[39m, \u001b[36mcx\u001b[39m, \u001b[36mhyout\u001b[39m, \u001b[36mcyout\u001b[39m all have size (H,B1,L)\n",
       "  for unidirectional RNNs, and (H,B1,2L) for bidirectional RNNs where B1 is\n",
       "  the size of the first minibatch."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc rnnforw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test loops\n",
    "function train(model,data,optim)\n",
    "    Σ,N=0,0\n",
    "    for (x,y,b) in data\n",
    "        grads,loss1 = lossgradient(model,x,y,b;pdrop=DROPOUT)\n",
    "        update!(model, grads, optim)\n",
    "        n = length(y)\n",
    "        Σ,N = Σ+n*loss1, N+n\n",
    "    end\n",
    "    return Σ/N\n",
    "end\n",
    "\n",
    "function test(model,data)\n",
    "    Σ,N=0,0\n",
    "    for (x,y,b) in data\n",
    "        loss1 = loss(model,x,y,b)\n",
    "        n = length(y)\n",
    "        Σ,N = Σ+n*loss1, N+n\n",
    "    end\n",
    "    return Σ/N\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63.418385 seconds (1.34 M allocations: 539.577 MiB, 19.29% gc time)\n",
      "  3.038773 seconds (49.12 k allocations: 3.443 MiB, 17.17% gc time)\n",
      "  1.796301 seconds (37.88 k allocations: 3.529 MiB, 14.45% gc time)\n",
      "(1, 740.4586f0, 1.2956388f6, 1.2024276f6)\n",
      " 63.598776 seconds (1.35 M allocations: 539.792 MiB, 17.97% gc time)\n",
      "  3.047761 seconds (49.12 k allocations: 3.443 MiB, 17.37% gc time)\n",
      "  1.800283 seconds (37.88 k allocations: 3.529 MiB, 14.47% gc time)\n",
      "(2, 578.68585f0, 55071.39f0, 51691.68f0)\n",
      " 63.619360 seconds (1.35 M allocations: 539.792 MiB, 18.00% gc time)\n",
      "  3.038184 seconds (49.12 k allocations: 3.443 MiB, 17.15% gc time)\n",
      "  1.803893 seconds (37.88 k allocations: 3.529 MiB, 14.53% gc time)\n",
      "(3, 420.61963f0, 32705.42f0, 31171.264f0)\n",
      " 63.832536 seconds (1.35 M allocations: 539.792 MiB, 18.23% gc time)\n",
      "  3.040227 seconds (49.12 k allocations: 3.443 MiB, 17.22% gc time)\n",
      "  1.799548 seconds (37.88 k allocations: 3.529 MiB, 14.46% gc time)\n",
      "(4, 354.73358f0, 9335.126f0, 8942.5205f0)\n",
      " 63.635407 seconds (1.35 M allocations: 539.792 MiB, 18.00% gc time)\n",
      "  3.034752 seconds (49.12 k allocations: 3.443 MiB, 17.11% gc time)\n",
      "  1.798584 seconds (37.88 k allocations: 3.529 MiB, 14.44% gc time)\n",
      "(5, 305.09595f0, 4181.1226f0, 3937.2559f0)\n",
      " 63.695399 seconds (1.35 M allocations: 539.792 MiB, 18.07% gc time)\n",
      "  3.045515 seconds (49.12 k allocations: 3.443 MiB, 17.33% gc time)\n",
      "  1.800915 seconds (37.88 k allocations: 3.529 MiB, 14.49% gc time)\n",
      "(6, 276.66925f0, 2553.5945f0, 2405.5078f0)\n",
      " 63.782057 seconds (1.35 M allocations: 539.792 MiB, 18.18% gc time)\n",
      "  3.043384 seconds (49.12 k allocations: 3.443 MiB, 17.26% gc time)\n",
      "  1.800487 seconds (37.88 k allocations: 3.529 MiB, 14.37% gc time)\n",
      "(7, 251.69386f0, 2197.9724f0, 2057.4202f0)\n",
      " 63.651095 seconds (1.35 M allocations: 539.792 MiB, 17.99% gc time)\n",
      "  3.035972 seconds (49.12 k allocations: 3.443 MiB, 17.10% gc time)\n",
      "  1.798600 seconds (37.88 k allocations: 3.529 MiB, 14.44% gc time)\n",
      "(8, 237.49559f0, 2453.2036f0, 2286.1477f0)\n",
      " 63.658497 seconds (1.35 M allocations: 539.792 MiB, 18.01% gc time)\n",
      "  3.035991 seconds (49.12 k allocations: 3.443 MiB, 17.07% gc time)\n",
      "  1.796418 seconds (37.88 k allocations: 3.529 MiB, 14.46% gc time)\n",
      "(9, 223.6597f0, 1639.319f0, 1521.0469f0)\n",
      " 63.595251 seconds (1.35 M allocations: 539.792 MiB, 17.96% gc time)\n",
      "  3.039084 seconds (49.12 k allocations: 3.443 MiB, 17.14% gc time)\n",
      "  1.801169 seconds (37.88 k allocations: 3.529 MiB, 14.39% gc time)\n",
      "(10, 208.67676f0, 1881.6254f0, 1735.9673f0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Tuple{Knet.RNN,KnetArray{Float32,3},KnetArray{Float32,2},KnetArray{Float32,2},KnetArray{Float32,2}}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using JLD to save the model\n",
    "model = optim = nothing; \n",
    "Knet.gc() # free gpu memory\n",
    "if !isfile(\"rnnlm.jld\")\n",
    "    # Initialize and train model\n",
    "    model = initmodel()\n",
    "    optim = optimizers(model,Adam,lr=LR,beta1=BETA_1,beta2=BETA_2,eps=EPS)\n",
    "    for epoch=1:EPOCHS\n",
    "        @time global j1 = train(model,mbtrn,optim)  # ~39 seconds\n",
    "        @time global j2 = test(model,mbval)         # ~1 second\n",
    "        @time global j3 = test(model,mbtst)         # ~1 second\n",
    "        println((epoch,exp(j1),exp(j2),exp(j3))); flush(stdout)  # prints perplexity = exp(negative_log_likelihood)\n",
    "    end\n",
    "    #save(\"rnnlm.jld\",\"model\",model)\n",
    "else\n",
    "    model = load(\"rnnlm.jld\",\"model\")\n",
    "    @time global j1 = test(model,mbtrn)\n",
    "    @time global j2 = test(model,mbval)\n",
    "    @time global j3 = test(model,mbtst)\n",
    "    println((EPOCHS,exp(j1),exp(j2),exp(j3))); flush(stdout)  # prints perplexity = exp(negative_log_likelihood)\n",
    "end\n",
    "summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
