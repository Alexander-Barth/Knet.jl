{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knet RNN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After installing and starting Julia run the following to install the required packages:\n",
    "# Pkg.init(); Pkg.update()\n",
    "# for p in (\"CUDAdrv\",\"IJulia\",\"PyCall\",\"JLD2\",\"Knet\"); Pkg.add(p); end\n",
    "# Pkg.checkout(\"Knet\",\"ilkarman\") # make sure we have the right Knet version\n",
    "# Pkg.build(\"Knet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /kuacc/users/dyuret/.julia/compiled/v1.0/Knet/f4vSz.ji for Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu() = 0\n"
     ]
    }
   ],
   "source": [
    "using Knet; @show gpu()\n",
    "True=true # so we can read the python params\n",
    "include(\"common/params_lstm.py\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: Linux\n",
      "Julia: 1.0.0\n",
      "GPU: Tesla K80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"OS: \", Sys.KERNEL)\n",
    "println(\"Julia: \", VERSION)\n",
    "#println(\"Knet: \", Pkg.installed(\"Knet\"))\n",
    "println(\"GPU: \", read(`nvidia-smi --query-gpu=name --format=csv,noheader`,String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "rnninit(inputSize, hiddenSize; opts...)\n",
       "```\n",
       "\n",
       "Return an `(r,w)` pair where `r` is a RNN struct and `w` is a single weight array that includes all matrices and biases for the RNN. Keyword arguments:\n",
       "\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0.0`: Dropout probability. Ignored if `numLayers==1`.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnninit(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\n",
       "  Return an \u001b[36m(r,w)\u001b[39m pair where \u001b[36mr\u001b[39m is a RNN struct and \u001b[36mw\u001b[39m is a single weight array that includes all matrices and biases for the RNN. Keyword arguments:\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0.0\u001b[39m: Dropout probability. Ignored if \u001b[36mnumLayers==1\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases\n",
       "  bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc rnninit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "function initmodel()\n",
    "    rnnSpec,rnnWeights = rnninit(EMBEDSIZE,NUMHIDDEN; rnnType=:gru)\n",
    "    inputMatrix = KnetArray(xavier(Float32,EMBEDSIZE,MAXFEATURES))\n",
    "    outputMatrix = KnetArray(xavier(Float32,2,NUMHIDDEN))\n",
    "    return rnnSpec,(rnnWeights,inputMatrix,outputMatrix)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "rnnforw(r, w, x[, hx, cx]; batchSizes, hy, cy)\n",
       "```\n",
       "\n",
       "Returns a tuple (y,hyout,cyout,rs) given rnn `r`, weights `w`, input `x` and optionally the initial hidden and cell states `hx` and `cx` (`cx` is only used in LSTMs).  `r` and `w` should come from a previous call to `rnninit`.  Both `hx` and `cx` are optional, they are treated as zero arrays if not provided.  The output `y` contains the hidden states of the final layer for each time step, `hyout` and `cyout` give the final hidden and cell states for all layers, `rs` is a buffer the RNN needs for its gradient calculation.\n",
       "\n",
       "The boolean keyword arguments `hy` and `cy` control whether `hyout` and `cyout` will be output.  By default `hy = (hx!=nothing)` and `cy = (cx!=nothing && r.mode==2)`, i.e. a hidden state will be output if one is provided as input and for cell state we also require an LSTM.  If `hy`/`cy` is `false`, `hyout`/`cyout` will be `nothing`. `batchSizes` can be an integer array that specifies non-uniform batch sizes as explained below. By default `batchSizes=nothing` and the same batch size, `size(x,2)`, is used for all time steps.\n",
       "\n",
       "The input and output dimensions are:\n",
       "\n",
       "  * `x`: (X,[B,T])\n",
       "  * `y`: (H/2H,[B,T])\n",
       "  * `hx`,`cx`,`hyout`,`cyout`: (H,B,L/2L)\n",
       "  * `batchSizes`: `nothing` or `Vector{Int}(T)`\n",
       "\n",
       "where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength, L is numLayers.  `x` can be 1, 2, or 3 dimensional.  If `batchSizes==nothing`, a 1-D `x` represents a single instance, a 2-D `x` represents a single minibatch, and a 3-D `x` represents a sequence of identically sized minibatches.  If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step in the sequence, in which case `sum(batchSizes)` should equal `div(length(x),size(x,1))`. `y` has the same dimensionality as `x`, differing only in its first dimension, which is H if the RNN is unidirectional, 2H if bidirectional.  Hidden vectors `hx`, `cx`, `hyout`, `cyout` all have size (H,B1,L) for unidirectional RNNs, and (H,B1,2L) for bidirectional RNNs where B1 is the size of the first minibatch.\n"
      ],
      "text/plain": [
       "\u001b[36m  rnnforw(r, w, x[, hx, cx]; batchSizes, hy, cy)\u001b[39m\n",
       "\n",
       "  Returns a tuple (y,hyout,cyout,rs) given rnn \u001b[36mr\u001b[39m, weights \u001b[36mw\u001b[39m, input \u001b[36mx\u001b[39m and optionally the initial hidden and cell states \u001b[36mhx\u001b[39m and \u001b[36mcx\u001b[39m (\u001b[36mcx\u001b[39m is only used in\n",
       "  LSTMs). \u001b[36mr\u001b[39m and \u001b[36mw\u001b[39m should come from a previous call to \u001b[36mrnninit\u001b[39m. Both \u001b[36mhx\u001b[39m and \u001b[36mcx\u001b[39m are optional, they are treated as zero arrays if not provided. The\n",
       "  output \u001b[36my\u001b[39m contains the hidden states of the final layer for each time step, \u001b[36mhyout\u001b[39m and \u001b[36mcyout\u001b[39m give the final hidden and cell states for all layers, \u001b[36mrs\u001b[39m\n",
       "  is a buffer the RNN needs for its gradient calculation.\n",
       "\n",
       "  The boolean keyword arguments \u001b[36mhy\u001b[39m and \u001b[36mcy\u001b[39m control whether \u001b[36mhyout\u001b[39m and \u001b[36mcyout\u001b[39m will be output. By default \u001b[36mhy = (hx!=nothing)\u001b[39m and \u001b[36mcy = (cx!=nothing &&\n",
       "  r.mode==2)\u001b[39m, i.e. a hidden state will be output if one is provided as input and for cell state we also require an LSTM. If \u001b[36mhy\u001b[39m/\u001b[36mcy\u001b[39m is \u001b[36mfalse\u001b[39m,\n",
       "  \u001b[36mhyout\u001b[39m/\u001b[36mcyout\u001b[39m will be \u001b[36mnothing\u001b[39m. \u001b[36mbatchSizes\u001b[39m can be an integer array that specifies non-uniform batch sizes as explained below. By default\n",
       "  \u001b[36mbatchSizes=nothing\u001b[39m and the same batch size, \u001b[36msize(x,2)\u001b[39m, is used for all time steps.\n",
       "\n",
       "  The input and output dimensions are:\n",
       "\n",
       "    •    \u001b[36mx\u001b[39m: (X,[B,T])\n",
       "\n",
       "    •    \u001b[36my\u001b[39m: (H/2H,[B,T])\n",
       "\n",
       "    •    \u001b[36mhx\u001b[39m,\u001b[36mcx\u001b[39m,\u001b[36mhyout\u001b[39m,\u001b[36mcyout\u001b[39m: (H,B,L/2L)\n",
       "\n",
       "    •    \u001b[36mbatchSizes\u001b[39m: \u001b[36mnothing\u001b[39m or \u001b[36mVector{Int}(T)\u001b[39m\n",
       "\n",
       "  where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength, L is numLayers. \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional. If \u001b[36mbatchSizes==nothing\u001b[39m, a\n",
       "  1-D \u001b[36mx\u001b[39m represents a single instance, a 2-D \u001b[36mx\u001b[39m represents a single minibatch, and a 3-D \u001b[36mx\u001b[39m represents a sequence of identically sized minibatches. If\n",
       "  \u001b[36mbatchSizes\u001b[39m is an array of (non-increasing) integers, it gives us the batch size for each time step in the sequence, in which case \u001b[36msum(batchSizes)\u001b[39m\n",
       "  should equal \u001b[36mdiv(length(x),size(x,1))\u001b[39m. \u001b[36my\u001b[39m has the same dimensionality as \u001b[36mx\u001b[39m, differing only in its first dimension, which is H if the RNN is\n",
       "  unidirectional, 2H if bidirectional. Hidden vectors \u001b[36mhx\u001b[39m, \u001b[36mcx\u001b[39m, \u001b[36mhyout\u001b[39m, \u001b[36mcyout\u001b[39m all have size (H,B1,L) for unidirectional RNNs, and (H,B1,2L) for\n",
       "  bidirectional RNNs where B1 is the size of the first minibatch."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc rnnforw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and its gradient\n",
    "function predict(weights, inputs, rnnSpec)\n",
    "    rnnWeights, inputMatrix, outputMatrix = weights # (1,1,W), (X,V), (2,H)\n",
    "    indices = permutedims(hcat(inputs...)) # (B,T)\n",
    "    rnnInput = inputMatrix[:,indices] # (X,B,T)\n",
    "    rnnOutput = rnnforw(rnnSpec, rnnWeights, rnnInput)[1] # (H,B,T)\n",
    "    return outputMatrix * rnnOutput[:,:,end] # (2,H) * (H,B) = (2,B)\n",
    "end\n",
    "\n",
    "loss(w,x,y,r)=nll(predict(w,x,r),y)\n",
    "lossgradient = grad(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading IMDB...\n",
      "└ @ Main /kuacc/users/dyuret/.julia/dev/Knet/data/imdb.jl:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10.077802 seconds (28.60 M allocations: 1.453 GiB, 7.55% gc time)\n",
      "25000-element Array{Array{Int32,1},1}\n",
      "25000-element Array{Int8,1}\n",
      "25000-element Array{Array{Int32,1},1}\n",
      "25000-element Array{Int8,1}\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "include(Knet.dir(\"data\",\"imdb.jl\"))\n",
    "@time (xtrn,ytrn,xtst,ytst,imdbdict)=imdb(maxlen=MAXLEN,maxval=MAXFEATURES)\n",
    "for d in (xtrn,ytrn,xtst,ytst); println(summary(d)); end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Array{String,1}:\n",
       " \"make\"    \n",
       " \"an\"      \n",
       " \"escape\"  \n",
       " \"back\"    \n",
       " \"to\"      \n",
       " \"base\"    \n",
       " \"br\"      \n",
       " \"br\"      \n",
       " \"later\"   \n",
       " \"they\"    \n",
       " \"are\"     \n",
       " \"sent\"    \n",
       " \"on\"      \n",
       " ⋮         \n",
       " \"honored\" \n",
       " \"as\"      \n",
       " \"he\"      \n",
       " \"is\"      \n",
       " \"made\"    \n",
       " \"an\"      \n",
       " \"honorary\"\n",
       " \"corporal\"\n",
       " \"in\"      \n",
       " \"the\"     \n",
       " \"british\" \n",
       " \"army\"    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbarray = Array{String}(undef,88584)\n",
    "for (k,v) in imdbdict; imdbarray[v]=k; end\n",
    "imdbarray[xtrn[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for training\n",
    "weights = nothing; Knet.gc(); # Reclaim memory from previous run\n",
    "rnnSpec,weights = initmodel()\n",
    "optim = optimizers(weights, Adam; lr=LR, beta1=BETA_1, beta2=BETA_2, eps=EPS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16.026946 seconds (17.51 M allocations: 2.671 GiB, 3.52% gc time)\n"
     ]
    }
   ],
   "source": [
    "# cold start\n",
    "@time for (x,y) in minibatch(xtrn,ytrn,BATCHSIZE;shuffle=true)\n",
    "    grads = lossgradient(weights,x,y,rnnSpec)\n",
    "    update!(weights, grads, optim)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `knetgc` is deprecated, use `Knet.gc` instead.\n",
      "│   caller = top-level scope at none:0\n",
      "└ @ Core none:0\n"
     ]
    }
   ],
   "source": [
    "# prepare for training\n",
    "weights = nothing; knetgc(); # Reclaim memory from previous run\n",
    "rnnSpec,weights = initmodel()\n",
    "optim = optimizers(weights, Adam; lr=LR, beta1=BETA_1, beta2=BETA_2, eps=EPS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training...\n",
      "└ @ Main In[16]:2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.893247 seconds (317.08 k allocations: 1.813 GiB, 0.99% gc time)\n",
      "  8.860284 seconds (319.04 k allocations: 1.813 GiB, 0.96% gc time)\n",
      "  8.862350 seconds (319.76 k allocations: 1.813 GiB, 0.88% gc time)\n",
      " 26.617344 seconds (957.56 k allocations: 5.441 GiB, 0.94% gc time)\n"
     ]
    }
   ],
   "source": [
    "# 29s\n",
    "@info(\"Training...\")\n",
    "@time for epoch in 1:EPOCHS\n",
    "    @time for (x,y) in minibatch(xtrn,ytrn,BATCHSIZE;shuffle=true)\n",
    "        grads = lossgradient(weights,x,y,rnnSpec)\n",
    "        update!(weights, grads, optim)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing...\n",
      "└ @ Main In[17]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.385483 seconds (1.71 M allocations: 144.006 MiB, 1.21% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8477163461538462"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info(\"Testing...\")\n",
    "@time accuracy(weights, minibatch(xtst,ytst,BATCHSIZE), (w,x)->predict(w,x,rnnSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
