{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been prepared based on the homework given in [berkeleydeeprl course](https://github.com/berkeleydeeprlcourse/homework/blob/master/sp17_hw/hw2/HW2.ipynb) and https://github.com/dennybritz/reinforcement-learning.\n",
    "This reviews the two classic methods for solving Markov Decision Processes (MDPs) with finite state and action spaces. We have value iteration (VI) and policy iteration (PI) for a finite MDP, both of which find the optimal policy in a finite number of iterations.\n",
    "\n",
    "The experiments here use the Frozen Lake environment, a simple gridworld MDP that is taken from gym. In this MDP, the agent must navigate from the start state to the goal state on a 4x4 grid, with stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# © Ozan Arkan Can, 2018.\n",
    "# In julia repl,\n",
    "# ENV[\"GYM_ENVS\"]=\"atari:classic_control:box2d\"\n",
    "# In pkg repl,\n",
    "# add Gym\n",
    "# build Gym\n",
    "using Gym, Random, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"FrozenLake-v0\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(env.gymenv[:unwrapped][:__doc__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1); seed!(env, 1);\n",
    "reset!(env)\n",
    "render(env)\n",
    "for t=1:100\n",
    "    a = sample(env.action_space)\n",
    "    ob, rew, done, _ = step!(env, a)\n",
    "    render(env)\n",
    "    if done\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the episode above, the agent falls into a hole after two time steps. Also note the stochasticity on the second step, the Right action is selected, but the agent moves to the down.\n",
    "\n",
    "We extract the relevant information from the gym env into the MDP struct below. The env object won't be used any further, we'll just use the mdp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct MDP\n",
    "    P # state transition and reward probabilities, explained below\n",
    "    nS # number of states\n",
    "    nA # number of actions\n",
    "    desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "end\n",
    "\n",
    "#We will modify the original transition matrix whicch allows more stable policies.\n",
    "#https://github.com/berkeleydeeprlcourse/homework/blob/60b3ef08c2dca3961efb63b32683bb588571f226/sp17_hw/hw2/frozen_lake.py#L89\n",
    "\n",
    "function inc(row, col, a)\n",
    "    if a==0 # left\n",
    "        col = max(col-1,0)\n",
    "    elseif a==1 # down\n",
    "        row = min(row+1,3)\n",
    "    elseif a==2 # right\n",
    "        col = min(col+1,3)\n",
    "    else # up\n",
    "        row = max(row-1,0)\n",
    "    end\n",
    "    return (row, col)\n",
    "end\n",
    "\n",
    "P = Dict(s => Dict(a => [] for a=1:4) for s=1:16)\n",
    "desc = map(x->convert(Array, x), env.gymenv[:unwrapped][:desc])\n",
    "for row=0:3\n",
    "    for col=0:3\n",
    "        s = row * 4 + col + 1\n",
    "        for a=1:4\n",
    "            li = P[s][a]\n",
    "            letter = desc[row+1][col+1]\n",
    "            if occursin(letter, \"GH\")\n",
    "                push!(li, (1.0, s, 0))\n",
    "            else            \n",
    "                for b in [(a+2)%4+1, a, a%4+1]\n",
    "                    newrow, newcol = inc(row, col, b-1)\n",
    "                    newstate = newrow * 4 + newcol + 1\n",
    "                    newletter = desc[newrow+1][newcol+1]\n",
    "                    reward = float(newletter == \"G\")\n",
    "                    push!(li, (b==a ? 0.8 : 0.1, newstate, reward))\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "mdp = MDP(P, env.gymenv[:unwrapped][:nS], env.gymenv[:unwrapped][:nA],desc);\n",
    "println(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "println(\"The 2D grid cells are associated with indices [1, 2, ..., 16] from left to right and top to down, as in\")\n",
    "println(\"[1 2 3 4\\n 5 6 7 8\\n 9 10 11 12\\n 13 14 15 16]\")\n",
    "println(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "println(\"For example, state 1 is the initial state, and the transition information for s=1, a=1 is \\nP[1][1] =\", mdp.P[1][1], \"\\n\")\n",
    "println(\"As another example, state 6 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\")\n",
    "println(\"P[6][1] =\", mdp.P[6][1], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the value iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "For $i=0, 1, 2, \\dots$\n",
    "- $V^{(i+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "We additionally define the sequence of greedy policies $\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}$, where\n",
    "$$\\pi^{(i)}(s) = \\arg \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$$\n",
    "\n",
    "The code returns two lists: $[V^{(0)}, V^{(1)}, \\dots, V^{(n)}]$ and $[\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}]$\n",
    "\n",
    "We choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by findmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Warning: We make a copy of your value function each iteration and use that copy for the update--instead of updating the value function in place. \n",
    "Updating in-place is also a valid algorithm, sometimes called Gauss-Seidel value iteration or asynchronous value iteration, but it will cause you to get different results than presented here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Inputs:\n",
    "        mdp: MDP\n",
    "        gamma: discount factor\n",
    "        nIt: number of iterations, corresponding to n above\n",
    "    Outputs:\n",
    "        (value_functions, policies)\n",
    "\"\"\"\n",
    "function value_iteration(mdp, gamma, nIt)\n",
    "    \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length mdp.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length mdp.nA containing the expected value of each action.\n",
    "    \"\"\"\n",
    "    function one_step_lookahead(state, V)\n",
    "        A = zeros(mdp.nA)\n",
    "        for a=1:mdp.nA\n",
    "            for (prob, next_state, reward) in mdp.P[state][a]\n",
    "                A[a] += prob * (reward + γ * V[next_state])\n",
    "            end\n",
    "        end\n",
    "        return A\n",
    "    end\n",
    "    \n",
    "    println(\"Iteration | max|V-Vprev| | # chg actions | V[1]\")\n",
    "    println(\"----------+--------------+---------------+---------\")\n",
    "    \n",
    "    Vs = [zeros(mdp.nS)] # list of value functions contains the initial value function V^{(0)}, which is zero\n",
    "    πs = []\n",
    "    for it=1:nIt\n",
    "        old_π = length(πs) > 0 ? πs[end] : nothing # \\pi^{(it)} = Greedy[V^{(it-1)}]. Just used for printout\n",
    "        Vprev = Vs[end] # V^{(it)}\n",
    "        # V: bellman backup on Vprev\n",
    "        #     corresponding to the math above: V^{(it+1)} = T[V^{(it)}]\n",
    "        #     array of floats\n",
    "        V = zeros(mdp.nS)\n",
    "        for s=1:mdp.nS\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, Vprev)\n",
    "            best_action_value = maximum(A)\n",
    "            # Update the value function\n",
    "            V[s] = best_action_value \n",
    "        end\n",
    "        \n",
    "        # π: greedy policy for Vprev, \n",
    "        #     corresponding to the math above: \\pi^{(it)} = Greedy[V^{(it)}]\n",
    "        #     array of ints\n",
    "        π = zeros(Int, mdp.nS)\n",
    "        for s=1:mdp.nS\n",
    "            # One step lookahead to find the best action for this state\n",
    "            A = one_step_lookahead(s, V)\n",
    "            #there might me more than one action with max value\n",
    "            #chose the first one\n",
    "            best_action = findmax(A)[2]\n",
    "            # Always take the best action\n",
    "            π[s] = best_action\n",
    "        end\n",
    "        \n",
    "        max_diff = maximum(abs.(V - Vprev))\n",
    "        nChgActions= old_π == nothing ? \"N/A\" : sum((π .!= old_π))\n",
    "        println(@sprintf(\"%4i      | %6.5f      | %4s          | %5.3f\", it, max_diff, nChgActions, V[1]))\n",
    "        push!(Vs, V)\n",
    "        push!(πs, π)\n",
    "    end\n",
    "    return Vs, πs\n",
    "end\n",
    "\n",
    "γ=0.95 # we'll be using this same value in subsequent implementations\n",
    "Vs_VI, πs_VI = value_iteration(mdp, γ, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Reshaped Grid Policy (1=left, 2=down, 3=right, 4=up)\")\n",
    "@show reshape(πs_VI[end], 4, 4)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Reshaped Grid Value Function\")\n",
    "@show reshape(Vs_VI[20], 4, 4)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Next we will implement the policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a V^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "Here, we implement a function called `policy_eval` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        π: [S] shaped vector representing the policy.\n",
    "        mdp: mdp.P represents the transition probabilities of the environment.\n",
    "            mdp.P[s][a] is a list of transition tuples (prob, next_state, reward).\n",
    "            mpd.nS is a number of states in the environment. \n",
    "            mdp.nA is a number of actions in the environment.\n",
    "        γ: discount factor.\n",
    "        θ: We stop evaluation once our value function change is less than theta for all states.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length mdp.nS representing the value function.\n",
    "\"\"\"\n",
    "function policy_eval(π, mdp, γ=0.95, θ=1e-4)\n",
    "    # Start with a random (all 0) value function\n",
    "    V = zeros(mdp.nS)\n",
    "    while true\n",
    "        Δ = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s=1:mdp.nS\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a in π[s]\n",
    "                action_prob = 1.0#Deterministic policy\n",
    "                # For each action, look at the possible next states...\n",
    "                for  (prob, next_state, reward) in mdp.P[s][a]\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + γ * V[next_state])\n",
    "                end\n",
    "            end\n",
    "            # How much our value function changed (across any states)\n",
    "            Δ = max(Δ, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        end\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if Δ < θ\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0.95\n",
    "θ = 1e-5\n",
    "random_policy = rand(1:mdp.nA, mdp.nS)\n",
    "V = policy_eval(random_policy, mdp, γ, θ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Reshaped Grid Policy (Random) (1=left, 2=down, 3=right, 4=up)\")\n",
    "@show reshape(random_policy, 4, 4)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Reshaped Grid Value Function\")\n",
    "@show reshape(V, 4, 4)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Policy Iteration Algorithm. Iteratively evaluates and improves a policy.\n",
    "    \n",
    "    Args:\n",
    "        mdp: A markov decision process.\n",
    "        γ: discount factor.\n",
    "        nIt: number of iteration\n",
    "        \n",
    "    Returns: (value_functions, policies)\n",
    "        \n",
    "\"\"\"\n",
    "function policy_iteration(mdp, γ, nIt)\n",
    "    \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length mdp.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length mdp.nA containing the expected value of each action.\n",
    "    \"\"\"\n",
    "    function one_step_lookahead(state, V)\n",
    "        A = zeros(mdp.nA)\n",
    "        for a=1:mdp.nA\n",
    "            for (prob, next_state, reward) in mdp.P[state][a]\n",
    "                A[a] += prob * (reward + γ * V[next_state])\n",
    "            end\n",
    "        end\n",
    "        return A\n",
    "    end\n",
    "    Vs = []\n",
    "    πs = []\n",
    "    π_prev = rand(1:mdp.nA, mdp.nS)#start with a randomly initialized policy\n",
    "    push!(πs, π_prev)\n",
    "    println(\"Iteration | # chg actions | V[1]\")\n",
    "    println(\"----------+---------------+---------\")\n",
    "    \n",
    "    for it=1:nIt\n",
    "        π_prev = πs[end]\n",
    "        Vπ = policy_eval(π_prev, mdp, γ)\n",
    "        π = zeros(Int, mdp.nS)\n",
    "        for s=1:mdp.nS\n",
    "            # One step lookahead to find the best action for this state\n",
    "            A = one_step_lookahead(s, Vπ)\n",
    "            #there might me more than one action with max value\n",
    "            #chose the first one\n",
    "            best_action = findmax(A)[2]\n",
    "            # Always take the best action\n",
    "            π[s] = best_action\n",
    "        end\n",
    "        nChgActions= sum((π .!= π_prev))\n",
    "        println(@sprintf(\"%4i      | %6i        | %6.5f\", it, nChgActions, Vπ[1]))\n",
    "        push!(Vs, Vπ)\n",
    "        push!(πs, π)\n",
    "    end\n",
    "    return Vs, πs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0.95\n",
    "Vs_PI, πs_PI = policy_iteration(mdp, γ, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Reshaped Grid Policy (1=left, 2=down, 3=right, 4=up)\")\n",
    "@show reshape(πs_PI[end], 4, 4)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Reshaped Grid Value Function\")\n",
    "@show reshape(Vs_PI[end], 4, 4)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In pkg repl,\n",
    "#add Plots\n",
    "using Plots; plotlyjs();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = reshape(map(x->\"V\"*string(x), 1:mdp.nS), 1, mdp.nS)\n",
    "plot(hcat(Vs_PI...)',label=labels, title=\"State Values for Policy Iteration\")\n",
    "xlabel!(\"Number of iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the convergence of value iteration and policy iteration on several states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots = []\n",
    "pallettes = [:blues, :reds, :greens, :blues, :reds]\n",
    "states = [1,9,11,14,15]\n",
    "for i=1:length(states)\n",
    "    s = states[i]\n",
    "    push!(plots, plot(hcat(hcat(Vs_VI[2:end]...)'[:, s], hcat(Vs_PI...)'[:, s]),\n",
    "            labels=[\"VI State $s\" \"PI State $s\"],\n",
    "            ylabel=\"V$s\", color_palette=pallettes[i], lw=4)\n",
    "    )\n",
    "end\n",
    "plot(plots...,layout=(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
